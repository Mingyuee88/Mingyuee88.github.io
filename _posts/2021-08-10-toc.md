---
title: Step 4 Inference
category: Jekyll
layout: post
---

# What is Inference?
Inference refers to the process in machine learning and deep learning where a trained model is used to process new input data and generate corresponding output. It is a critical phase of model application that typically occurs after training is completed.

# The Inference Process
The inference process generally consists of the following steps:

## 1. Input Processing
- **Data Preparation**: Transform raw input data into a format that the model can understand. This may involve data cleaning, normalization, and feature extraction.
- **Encoding**: Convert textual input into numerical representations (e.g., word embeddings).

## 2. Model Inference
- **Forward Propagation**: Pass the processed input through the model, which computes its output through its layered architecture.
- **Activation Functions**: The output of each neuron is passed through an activation function to introduce non-linearity, resulting in the final model output.

## 3. Generating Output
- **Text Generation**: For generative models (like GPT-2), the model generates text based on the input.
- **Prediction Results**: For classification models (like image classifiers), the output is the corresponding class label or probability distribution.

## 4. Result Post-processing
- **Decoding**: Convert the model's output back into a human-readable format. For instance, converting generated text from numerical form to strings.
- **Evaluation**: Assess the accuracy and relevance of the output based on specific requirements.

# Applications of Inference
Inference has a wide range of applications across various fields, including but not limited to:

- **Natural Language Processing**: Text generation, machine translation, sentiment analysis, etc.
- **Computer Vision**: Image recognition, object detection, image segmentation, etc.
- **Speech Recognition**: Speech-to-text, virtual assistants, etc.

# Demo: Reproducing OpenAI's GPT-2

## Overview
GPT-2 was published by OpenAI in 2019, showcasing advancements in natural language processing. The foundational paper is titled "Language Models are Unsupervised Multitask Learners."

## Key Features of GPT-2
- **Model Architecture**: 
  - Transformer neural network with 1.6 billion parameters.
  - Maximum context length of 1024 tokens.
  - Trained on about 100 billion tokens.

## Reproduction with llm.c
For further details, visit the discussion here: [Let's Reproduce GPT-2](https://github.com/karpathy/llm.c/discussions/677).

# "Base" Models in the Wild

## Examples of Base Models
- **OpenAI GPT-2 (2019)**: 
  - **Parameters**: 1.6 billion 
  - **Trained on**: 100 billion tokens

- **Llama 3 (2024)**: 
  - **Parameters**: 405 billion 
  - **Trained on**: 15 trillion tokens
  - [Llama 3 paper from Meta](https://arxiv.org/abs/2407.21783)

## What is a Release of a Model?
The components that are made available when a machine learning model is published or shared:
- **The Code**: 
   - The code required to run the Transformer (e.g., approximately 200 lines of code in Python).
- **Model Parameters**: 
   - The parameters of the Transformer (e.g., 1.6 billion numbers).

# The Psychology of a Base Model

- It is a token-level internet document simulator.
- It is stochastic/probabilisticâ€”you're going to get different results each time you run it.
- It "dreams" internet documents.
- It can also recite some training documents verbatim from memory ("regurgitation").
- The parameters of the model are like a lossy zip file of the internet.
  - This serves as a way to store a lot of useful knowledge within the parameters of the network.
- You can already use it for applications (e.g., translation) by being clever with your prompts.
  - Example: English-Korean translator by constructing a "few-shot" prompt and leveraging "in-context learning" ability.
  - Example: An Assistant that answers questions using a prompt that resembles a conversation.
  
- But we can do better...