---
title: Summary  
author: Tao He  
date: 2023-10-14  
category: Jekyll  
layout: post  
mermaid: true  
---  

In this website, we have explored the entire process of self-learning large language models (LLMs), from pre-training to reinforcement learning. Here’s a summary of what we’ve learned:

# 1. Pre-Training Stage

Pre-training is the foundational stage of large language models. By learning from a vast amount of data, the model masters the basic structure and patterns of language. This stage includes several important steps:

**1.1 Data Download and Pre-Processing:** We start by collecting and processing a large corpus of text data to prepare for subsequent training.
  
**1.2 Tokenization:** The text is then tokenized to convert it into an input format that the model can understand.
  
**1.3 Neural Network Training:** Through training the neural network, the model learns language patterns and regularities present in the text data.
  
**1.4 Inference:** Once training is complete, the model can perform inference and generate output based on input text.

Additionally, this stage includes exploring and understanding foundational models such as GPT-2, which serve as a starting point for building more powerful models.

# 2. Post-Training

**2.1 Supervised Fine-Tuning**

The fine-tuning stage is crucial for optimizing the model further. In this phase, the pre-trained model is adjusted according to the specific task requirements, allowing it to better adapt to tasks such as text generation and question answering by training on labeled datasets.

**2.2 Reinforcement Learning**

Reinforcement learning plays a vital role in LLM development, especially in dynamic environments and complex problem-solving. We introduced the basic concepts of reinforcement learning and how it enhances the model's performance and adaptability.

**2.3 Reinforcement Learning with Human Feedback (RLHF)**

In this process, the model benefits not only from algorithmic feedback but also from human feedback. By integrating reinforcement learning with human insights, the model gradually learns to provide more reasonable and accurate responses in practical applications.


Through the study, we realize that LLMs utilize not only traditional machine learning algorithms but also deep learning and reinforcement learning. To give you a clearer picture of where LLM fit within the broader field of Machine Learning, refer to the mind map below. It illustrates the relationship between traditional machine learning algorithms, deep learning, and large language models.

![Machine Learning](https://github.com/user-attachments/assets/b09a5138-b03d-4bf2-842f-d33ecd3192ca)
